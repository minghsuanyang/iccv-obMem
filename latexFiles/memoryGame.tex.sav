To measure the memorability of individual objects from our dataset, we created an alternate version of the Visual Memory Game following the basic design in \cite{isola11}, with the exception of a few key differences. We administered the game and collected data through Amazon Mechanical Turk. In our game, participants first viewed a sequence of images one at a time, with a 1.5 second gap in between image presentations. Subjects were asked to remember the contents and objects inside those images as much as they could. To ensure that subjects would not just only look at the salient or center objects, subjects had unlimited time to freely view the images. Once they were done viewing an image, they could press any key to advance to the next image. Following the initial sequence, participants then viewed a sequence of objects, their task then being to indicate through a key press which of those objects was present in one of the previously shown images.


In order to measure image memorability, we presented workers on Amazon Mechanical Turk with a Visual Memory Game. In the game, participants viewed a sequence of images, each of which was displayed for 1 second, with a 1.4 second gap in between image presentations (Figure 3). Their task was to press the space bar whenever they saw an identical repeat of an image at any time in the sequence [1] [12]. Participants received feedback whenever they pressed a key (a green symbol shown at the center of the screen for correct detection, and a gray X for an error). Image sequences were broken up into levels that consisted of 120 images each. Each level took 4.8 minutes to perform. At the end of each level, the participant saw his or her correct response average score for that level, and was allowed to take a short break. Participants could complete at most 30 levels, and were able to exit the game at any time. A total of 665 workers from Mechanical Turk (> 95% approval rate in Amazon’s system) performed the game. Over 90% of our data came from 347 of these workers. We payed workers $0.30 per level in proportion to the amount of the level completed, plus a $0.10 bonus per fully completed level. This adds up to about $5 per hour. The average worker stayed in the game for over 13 levels. Unbeknownst to the participants, the sequence of images was composed of ‘targets’ (2222 images) and ‘fillers’ (8220 images). Target and filler images represented a random sampling of the scene categories from the SUN dataset
[24]. All images were scaled and cropped about their centers to be 256x256 pixels. The role of the fillers was twofold: first, they provided spacing between the first and second
repetition of a target; second, responses on repeated fillers constituted a ‘vigilance task’ that allowed us to continuously check that participants were attentive to the task
[1, 12]. Repeats occurred on the fillers with a spacing of 1-7 images, and on the targets with a spacing of 91-109 images. Each target was sequenced to repeat exactly once, and each
filler was presented at most once, unless it was a vigilance task filler, in which case it was sequenced to repeat exactly once. Stringent criteria were used to continuously screen
worker performance once they entered the game. First, the game automatically ended whenever a participant fell below a 50% success rate on the last 10 vigilance task repeats
or above a 50% error rate on the last 30 non-repeat images. When this happened, all data collected on the current level was discarded. Rejection criterion reset after each level. If
a participant failed any of the vigilance criteria, they were flagged. After receiving three such flags they were blocked from further participation in the experiment. Otherwise,
participants were able to restart the game as many times as they wished until completing the max 30 levels. Upon each restart, the sequence was reset so that the participant
would never see an image they had seen in a previous session. Finally, a qualification and training ‘demo’ preceeded the actual memory game levels. After collecting the data, we assigned a ‘memorability score’ to each image, defined as the percentage of correct detections by participants. On average, each image was scored by 78 participants. The average memorability score
was 67.5% (SD of 13.6%). Average false alarm rate was 10.7% (SD of 7.6%). As the false alarm rate was low in comparison with correct detections, correct detections are unlikely to be lucky confusions. Specifically, we expect lucky confusions to account for no more than 10.7% of correct detections on average. Therefore, we believe our memorability scores are a good measure of correct memories.



To measure the memorability of individual objects from our dataset, we created an alternate version of the Visual Memory Game following the basic design of [Oliva, CVPR 2011], with the exception of a few key differences. We administered the game and collected data through Amazon Mechanical Turk. In our game, participants first viewed a sequence of images one at a time. Following the initial sequence, participants then viewed a sequence of objects, their task then being to indicate through a key press which of those objects was present in one of the previously shown images. Pairs of corresponding image and object sequences were broken up into 10 blocks. Each block consisted of 80 total images, and lasted approximately 3 minutes. Each sequence of images was pseudo-random and contained of 3 target images which contained a number of objects that participants were to later identify. The remaining images in the sequence consisted of 16 filler images and 16 familiar images. Successive target images were spaced 70-79 images apart, and familiar images were spaced 35-79 image apart. The corresponding sequence of objects contained 3 segmented target objects, one for each of the target images shown previously, along with 16 filler objects, 16 familiar objects, and 10 control objects. Target objects appeared only once, and participants were tested on only one object from each target image. Each target object was spaced X-X objects apart. Objects were centered within their parent frame and non-object pixels were set to grey. Participants were required to complete the entire task, which lasted approximately 30 minutes, and could not participate in the experiment a second time. A total of 2000 workers completed the task, meaning around 20 participants were given the opportunity to remember each object. 